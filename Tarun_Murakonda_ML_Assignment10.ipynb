{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarun_Murakonda_ML Assignment10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalil192/synerzy-android-client/blob/master/Tarun_Murakonda_ML_Assignment10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ije14bJbmH",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 1 : Say you are standing at the bottom of a staircase with  a dice. With each throw of the dice you either move down one step (if you get a 1 or 2 on the dice) or move up one step (if you get a 3, 4 or 5 on the dice). If you throw a 6 on the dice, you throw the dice again and move up the staircase by the number you get on that second throw. Note if you are on the base of the staircase you cannot move down! What is the probability that you will reach more than 60 steps after 250 throws of the dice. Change the code so that you have a function that takes as parameter, the number of throws\n",
        "Add a new parameter to the function that takes a probability distribution over all outcomes from a dice throw. For example (0.2,0.3,0.2,0.1,0.1,0.1) would suggest that the probability of getting a 1 is 0.2, 2 is 0.3 etc. How does that change the probability of reaching a step higher than 60?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGzwczIoYjkN",
        "colab_type": "code",
        "outputId": "987b1b6a-8e35-4c0f-a5f2-1fba65ce23b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "moves = [1,2,3,4,5,6]\n",
        "prob = [0.2,0.3,0.2,0.1,0.1,0.1]\n",
        "\n",
        "def rollDice():\n",
        "    current_move = np.random.choice(a= moves , p = prob)\n",
        "    return int(current_move) \n",
        "\n",
        "Iterations = 10000\n",
        "down_movement = [1,2]\n",
        "up_movement = [3,4,5]\n",
        "successful_iterations = 0\n",
        "selected6 = 0\n",
        "for i in range (0,Iterations): \n",
        "    total_steps = 250\n",
        "    current_position = 0\n",
        "    selected6 = 0\n",
        "    completedMoves = 0\n",
        "    while completedMoves < total_steps:\n",
        "        completedMoves +=1\n",
        "        if(current_position > 60):\n",
        "            successful_iterations+=1\n",
        "            break\n",
        "        current_movement = rollDice()\n",
        "        if(current_movement in  down_movement):\n",
        "            current_position -=1\n",
        "        elif current_movement in up_movement :\n",
        "            current_position +=1    \n",
        "        else  :#current movement  = 6\n",
        "            completedMoves -=1\n",
        "            current_movement = rollDice()\n",
        "            current_position += current_movement\n",
        "        \n",
        "probabilty_of_reaching = successful_iterations/Iterations\n",
        "print(successful_iterations)\n",
        "print(probabilty_of_reaching)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4300\n",
            "0.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiW7kkuBJuv8",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 2 : . Generate random data for for Multiple Linear Regression, Logistic Regression, K-mean Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLXx8PLaYld_",
        "colab_type": "code",
        "outputId": "292c6cd1-8b11-47c0-cf2c-2e5de45f442b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#randomly generated data for the Multiple Linear regression \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for p in range(n_features):\n",
        "  X_p = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_p)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Random data for the Logistic regression\n",
        "n_features_of_the_model = 4\n",
        "X = []\n",
        "for i in range(n_features_of_the_model):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "#Random data for K-mean clustering\n",
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0 -0.277302  0.472816  0.416761 -2.364841 -0.142300\n",
            "1 -0.504426 -0.776690  0.140841  0.139434  0.512164\n",
            "2 -0.239787  0.814096 -0.261044  0.172356  1.492457\n",
            "3 -0.026431 -0.099781  2.088728 -0.224606  1.282512\n",
            "4 -1.801184 -0.899966 -0.437503  0.100811 -0.481579\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.646858 -0.281309 -0.822020  0.690552  1.372156\n",
            "96  0.335225  0.218647 -1.288144  0.012648  1.056282\n",
            "97  0.108906  1.066057 -0.873900  0.165173  1.064266\n",
            "98  0.565440 -1.101300 -0.830166  1.714917  1.576807\n",
            "99 -2.299296  0.901309 -0.711108 -0.925546 -0.671608\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.052257    0.141579    0.074845   -0.039480    1.060473\n",
            "std      0.935180    0.926148    0.869520    0.947450    0.885940\n",
            "min     -2.520938   -1.801890   -1.969227   -2.364841   -1.464196\n",
            "25%     -0.387588   -0.496477   -0.602850   -0.669114    0.507709\n",
            "50%      0.126632    0.190000    0.191192    0.038043    1.082238\n",
            "75%      0.619795    0.641178    0.671436    0.552921    1.588872\n",
            "max      2.195408    2.887494    2.088728    2.501401    3.428952\n",
            "         X0        X1        X2        X3         Y\n",
            "0 -0.277302  0.472816  0.416761 -2.364841 -0.142300\n",
            "1 -0.504426 -0.776690  0.140841  0.139434  0.512164\n",
            "2 -0.239787  0.814096 -0.261044  0.172356  1.492457\n",
            "3 -0.026431 -0.099781  2.088728 -0.224606  1.282512\n",
            "4 -1.801184 -0.899966 -0.437503  0.100811 -0.481579\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.646858 -0.281309 -0.822020  0.690552  1.372156\n",
            "96  0.335225  0.218647 -1.288144  0.012648  1.056282\n",
            "97  0.108906  1.066057 -0.873900  0.165173  1.064266\n",
            "98  0.565440 -1.101300 -0.830166  1.714917  1.576807\n",
            "99 -2.299296  0.901309 -0.711108 -0.925546 -0.671608\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.052257    0.141579    0.074845   -0.039480    1.060473\n",
            "std      0.935180    0.926148    0.869520    0.947450    0.885940\n",
            "min     -2.520938   -1.801890   -1.969227   -2.364841   -1.464196\n",
            "25%     -0.387588   -0.496477   -0.602850   -0.669114    0.507709\n",
            "50%      0.126632    0.190000    0.191192    0.038043    1.082238\n",
            "75%      0.619795    0.641178    0.671436    0.552921    1.588872\n",
            "max      2.195408    2.887494    2.088728    2.501401    3.428952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0 -0.277302  0.472816  0.416761 -2.364841 -0.142300\n",
            "1 -0.504426 -0.776690  0.140841  0.139434  0.512164\n",
            "2 -0.239787  0.814096 -0.261044  0.172356  1.492457\n",
            "3 -0.026431 -0.099781  2.088728 -0.224606  1.282512\n",
            "4 -1.801184 -0.899966 -0.437503  0.100811 -0.481579\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.646858 -0.281309 -0.822020  0.690552  1.372156\n",
            "96  0.335225  0.218647 -1.288144  0.012648  1.056282\n",
            "97  0.108906  1.066057 -0.873900  0.165173  1.064266\n",
            "98  0.565440 -1.101300 -0.830166  1.714917  1.576807\n",
            "99 -2.299296  0.901309 -0.711108 -0.925546 -0.671608\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.052257    0.141579    0.074845   -0.039480    1.060473\n",
            "std      0.935180    0.926148    0.869520    0.947450    0.885940\n",
            "min     -2.520938   -1.801890   -1.969227   -2.364841   -1.464196\n",
            "25%     -0.387588   -0.496477   -0.602850   -0.669114    0.507709\n",
            "50%      0.126632    0.190000    0.191192    0.038043    1.082238\n",
            "75%      0.619795    0.641178    0.671436    0.552921    1.588872\n",
            "max      2.195408    2.887494    2.088728    2.501401    3.428952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0pQVHsrKLAB",
        "colab_type": "text"
      },
      "source": [
        "Problem 3: a)Linear regression using gradient descent                         \n",
        "                       b)Logistic regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuvSd0F2Yq6W",
        "colab_type": "code",
        "outputId": "5f31cf28-6607-4fb1-d2c7-f1bb399c381c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "#linear regression using the gradient descent\n",
        "print(\" linear regrission using gradient descent are\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#logistic regression using gradient descent\n",
        "print(\"from logistic using gradient descent are\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameter constants that we got from the linear regrission using gradient descent are\n",
            "0.09886833802151068 0.1919101578263811\n",
            "\n",
            "\n",
            "\n",
            "from logistic using gradient descent are\n",
            "0.6931471805599453\n",
            "0.4391663577070409\n",
            "0.4386779245740476\n",
            "0.4382041851128081\n",
            "0.43774475332561014\n",
            "0.4372992510256304\n",
            "0.4368673076675682\n",
            "0.4364485601920152\n",
            "0.4360426528832192\n",
            "0.4356492372396484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4oNn-oLQHZ",
        "colab_type": "text"
      },
      "source": [
        "c)Linear Regression using L1 and L2 regularization                              \n",
        "d)Logistic regression using L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL2fLbxsYvVY",
        "colab_type": "code",
        "outputId": "dd8206d8-33f2-482c-d4de-a51845105da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "print(\"Linear Regression using L1 and L2 regularization\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"Linear regression using L2 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L1 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L2 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear regression using L1 regularisation\n",
            "0.08968123956971578 0.19195598683479365\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Linear regression using L2 regularisation\n",
            "0.09839242845495579 0.1919117551965817\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L1 regularisation\n",
            "0.6931471805599453\n",
            "0.042575623774034566\n",
            "-0.3477422107092938\n",
            "-0.7313711437190957\n",
            "-1.1084185637838995\n",
            "-1.4789930578214068\n",
            "-1.8432041482285326\n",
            "-2.2011620438546835\n",
            "-2.552977404707571\n",
            "-2.8987611202176247\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L2 regularisation\n",
            "0.6931471805599453\n",
            "0.43966743355786586\n",
            "0.44063261501246814\n",
            "0.44249364071367386\n",
            "0.4451826380682229\n",
            "0.4486354739926612\n",
            "0.4527915818786326\n",
            "0.45759379567792563\n",
            "0.4629881907730305\n",
            "0.4689239313228966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FqHJruLrjV",
        "colab_type": "text"
      },
      "source": [
        "e)K-means Clustering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRkhDBZY7MY",
        "colab_type": "code",
        "outputId": "30383fc7-3778-4f27-f8de-c733a503eb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaBVxF6RL4CU",
        "colab_type": "text"
      },
      "source": [
        "Problem 4 :Linear Regression from scratch using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MBIqfDuZAKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2AiZS-eMFRR",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37AURcbRZBcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlW7KyNOMAcy",
        "colab_type": "text"
      },
      "source": [
        "K means from scratch using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODgvm-8nZFVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}